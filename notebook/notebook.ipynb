{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to run\n",
        "Run all cells using the combination CTRL + F9, images will be visualized using the best trained model \"updatedmodel\", the images will be outputed in the \"Visualizing images\" section. You can change the hyperparameters for the training in the hyperparameters sections, just set the variable TRAIN to True."
      ],
      "metadata": {
        "id": "eQoZ5wmzRYeK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjmmMP5cc24I"
      },
      "source": [
        "### ENVIROMENT SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yCOvNht2f8Z"
      },
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/CristianTinaburri/ImageColorizationVisionAndPerception\n",
        "\n",
        "# Copy contents\n",
        "!cp -a /content/ImageColorizationVisionAndPerception/. /content/\n",
        "\n",
        "# Clean Enviroment\n",
        "!rm -rf /content/ImageColorizationVisionAndPerception\n",
        "!rm -rf /content/sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BqYIpAFG9rsn"
      },
      "outputs": [],
      "source": [
        "# Clean folders\n",
        "!rm -rf /content/training_images\n",
        "!rm -rf /content/testing_images\n",
        "\n",
        "!mkdir /content/training_images\n",
        "!mkdir /content/testing_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsMF5AqXc0cH"
      },
      "source": [
        "### LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z6edOx2GZ_rl"
      },
      "outputs": [],
      "source": [
        "# LIBRARIES\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from skimage import color\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import cv2\n",
        "\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCzW5cpBH3K7"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RksKtBuzH5K7"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 256       # image size\n",
        "num_epochs = 250       # number of epochs to be performed during training\n",
        "batch_size = 50        # batch size\n",
        "learning_rate = 2e-4      # learning rate\n",
        "betas = (0.9, 0.999)      # betas\n",
        "eps = 1e-08       # eps\n",
        "weight_decay = 2.5e-5     # weight_decay\n",
        "\n",
        "use_gpu = True          # use_gpu\n",
        "num_workers = 2          # num_workers\n",
        "TRANSFORM_DATA = True      # apply transform to data\n",
        "TRANSFORM_DATA_MECHANICALLY = False # apply image transformation on dataset\n",
        "DATASET = \"CIFAR\"          # the dataset you want to use for training\n",
        "MODEL_NAME = \"updated_model\" + \".pth\"    # name of model to save\n",
        "MODEL_PATH_SAVE = \"/content/gdrive/MyDrive/Computer Vision Project/Models/End Models/\" + MODEL_NAME + \"/\" # where to save model\n",
        "NUM_EPOCH_MODEL_SAVE = 5       # how many epoch to save model\n",
        "TEST_EPOCHS = 10        # the number of epoch after it need to be tested\n",
        "LOAD_MODEL = True   # if the model needs to be loaded\n",
        "TRAIN = False   # if the model needs to be trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "607n-w1mGNGg"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(MODEL_PATH_SAVE):\n",
        "  os.mkdir(MODEL_PATH_SAVE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOYFDUorctA1"
      },
      "source": [
        "### Dataset Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m3H_6py1HYTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4145cfc-3c74-42dd-a640-33981d84a86c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/modules\n",
            "/content\n",
            "5000\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "%cd modules\n",
        "\n",
        "import utils\n",
        "\n",
        "%cd ..\n",
        "\n",
        "utils.copy_dataset_for_training('datasets/' + DATASET)\n",
        "number_files_training, number_files_testing = utils.get_number_of_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XqVf8TWr3Lv5"
      },
      "outputs": [],
      "source": [
        "if TRANSFORM_DATA == True:\n",
        "  transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Resize((IMG_SIZE,IMG_SIZE)), transforms.ToPILImage()])\n",
        "else:\n",
        "  transform = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JG0rs9xbMxo",
        "outputId": "1482dd2a-e161-4481-a4ee-e4268321b4a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/modules\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd modules\n",
        "\n",
        "import dataset\n",
        "\n",
        "# Setup data\n",
        "train_set = dataset.ImageDataset(\"/content/training_images\", transform = transform)\n",
        "test_set = dataset.ImageDataset(\"/content/testing_images\", transform = transform)\n",
        "\n",
        "# Setup dataloaders\n",
        "train_dataloader = DataLoader(train_set, num_workers=num_workers, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_set, num_workers=num_workers, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yFBqUScrv4"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAI3WRKpxxt6",
        "outputId": "9fd8d043-9424-4f6b-f3b9-7c47aa9a5846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# THIS CELL IMPORTS THE MODELS\n",
        "\n",
        "%cd models\n",
        "\n",
        "import colorfulcolorization\n",
        "import basemodel\n",
        "import updatedmodel\n",
        "import colornetblock\n",
        "import ResNetBackbone18\n",
        "\n",
        "colorfulcolorization = colorfulcolorization.colorfulcolorization()\n",
        "basemodel = basemodel.basemodel()\n",
        "updatedmodel = updatedmodel.updatedmodel()\n",
        "ResNetBackbone18 = ResNetBackbone18.ResNetBackbone18(p=0.8)\n",
        "colornetblock = colornetblock.colornetblock(dim=128, percentage_dropout=0.5)\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEC-KC5__gJv",
        "outputId": "a0f7cf80-d421-470e-aa84-fa821f4d3494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 128104\n"
          ]
        }
      ],
      "source": [
        "# THIS CELL INITIALIZE THE CHOOSEN MODEL\n",
        "\n",
        "# Initialize model\n",
        "# CHANGE THIS LINE TO THE MODEL ARCHITECTURE YOU WANT\n",
        "model = updatedmodel\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Port model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Print number of parameters\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % (num_params))\n",
        "\n",
        "# Import libraries to get summary of model architecture\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "\n",
        "# Get summary of model architecture\n",
        "# summary(model, (1,32,32))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL == True:\n",
        "  MODEL_PATH_TO_LOAD = \"/content/models/pretrained/\" + MODEL_NAME\n",
        "  model.load_state_dict(copy.deepcopy(torch.load(MODEL_PATH_TO_LOAD, device)))\n",
        "  model.eval()"
      ],
      "metadata": {
        "id": "TJ7su07gKrNS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hxbcHbJ8Rh_"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0h1BpWI8khY"
      },
      "outputs": [],
      "source": [
        "# Loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, betas=betas, eps=eps, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjul7c-QdrNw"
      },
      "outputs": [],
      "source": [
        "if TRAIN == True:\n",
        "  # set to training mode\n",
        "  model.train()\n",
        "\n",
        "  train_loss_avg = []\n",
        "\n",
        "  print('Training Started')\n",
        "  for epoch in range(num_epochs):\n",
        "      train_loss_avg.append(0)\n",
        "      num_batches = 0\n",
        "\n",
        "      if epoch % NUM_EPOCH_MODEL_SAVE == 0 and epoch != 0:\n",
        "        torch.save(model.state_dict(), MODEL_PATH_SAVE + MODEL_NAME + \"_\" + str(int(epoch)) + \".pth\")\n",
        "        pass\n",
        "\n",
        "      if epoch % 10 == 0 and epoch != 0:\n",
        "        utils.save_predictions()\n",
        "        pass\n",
        "      \n",
        "      for l, ab in train_dataloader:\n",
        "          \n",
        "          l, ab = l.to(device), ab.to(device)\n",
        "\n",
        "          predicted_ab_batch = model(l)\n",
        "          \n",
        "          loss = loss_function(predicted_ab_batch, ab)\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          \n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss_avg[-1] += loss.item()\n",
        "          num_batches += 1\n",
        "\n",
        "          if num_batches*batch_size % int(number_files_training/10) == 0 and num_batches > 0:\n",
        "            print(num_batches*batch_size, \" / \", number_files_training)\n",
        "          \n",
        "      train_loss_avg[-1] /= num_batches\n",
        "      print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))\n",
        "\n",
        "  torch.save(model.state_dict(), MODEL_PATH_SAVE + MODEL_NAME + \"_\" + str(int(epoch)) + \".pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4WfclFA7wYt"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqDv2hk-8UFX",
        "outputId": "6c3d16a3-8ee8-4871-909b-cc8539879b27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average loss: 91.890744\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "test_loss_avg, num_batches = 0, 0\n",
        "\n",
        "for l, ab in test_dataloader:\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        l, ab = l.to(device), ab.to(device)\n",
        "\n",
        "        predicted_ab_batch = model(l)\n",
        "        \n",
        "        loss = loss_function(predicted_ab_batch, ab)\n",
        "\n",
        "        test_loss_avg += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "test_loss_avg /= num_batches\n",
        "print('average loss: %f' % (test_loss_avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhBwvatXeDTF"
      },
      "source": [
        "### Visualizing images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0XnjNP0eB8O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from skimage import color, io\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "import torchvision.utils\n",
        "\n",
        "number_of_images_grid = 5\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # BATCH DI TEST DI 10 IMMAGINI\n",
        "    # ritorna un array con 10 numeri casuali \n",
        "    image_inds = np.random.choice(len(test_set), number_of_images_grid, replace=False)\n",
        "\n",
        "    # ritorna un tensore cone i 10 elementi casuali\n",
        "    lab_batch = torch.stack([torch.cat([test_set[i][0], test_set[i][1]], dim=0) for i in image_inds])\n",
        "\n",
        "    # porta il lab_batch su gpu\n",
        "    lab_batch = lab_batch.to(device)\n",
        "\n",
        "    # dichiaro lista per le immagini da predirre\n",
        "    predicted_lab_batch = []\n",
        "\n",
        "    # predice il canale l dell'immagine\n",
        "    predicted_lab_batch = torch.cat([lab_batch[:, 0:1, :, :], model(lab_batch[:, 0:1, :, :])], dim=1)\n",
        "\n",
        "    # porta il batch sulla cpu\n",
        "    lab_batch = lab_batch.cpu()\n",
        "\n",
        "    # porta la batch predetta sulla cpu\n",
        "    predicted_lab_batch = predicted_lab_batch.cpu()\n",
        "\n",
        "    rgb_batch = []\n",
        "    predicted_rgb_batch = []\n",
        "    for i in range(lab_batch.size(0)):\n",
        "        predicted_rgb_img = color.lab2rgb(np.transpose(predicted_lab_batch[i, :, :, :].numpy().astype('float64'), (1, 2, 0)))\n",
        "        predicted_rgb_batch.append(torch.FloatTensor(np.transpose(predicted_rgb_img, (2, 0, 1))))\n",
        "\n",
        "        rgb_img = color.lab2rgb(np.transpose(lab_batch[i, :, :, :].numpy().astype('float64'), (1, 2, 0)))\n",
        "        rgb_batch.append(torch.FloatTensor(np.transpose(rgb_img, (2, 0, 1))))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(30, 30), nrows=1, ncols=2)\n",
        "    ax[0].imshow(np.transpose(torchvision.utils.make_grid(torch.stack(predicted_rgb_batch), nrow=5).numpy(), (1, 2, 0)))\n",
        "    ax[0].title.set_text('Predicted')\n",
        "    ax[1].imshow(np.transpose(torchvision.utils.make_grid(torch.stack(rgb_batch), nrow=5).numpy(), (1, 2, 0)))\n",
        "    ax[1].title.set_text('Reference')\n",
        "\n",
        "    # plt.savefig(MODEL_PATH_SAVE + MODEL_NAME + '_' + str(250) + '.png', bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LjmmMP5cc24I",
        "WsMF5AqXc0cH",
        "XCzW5cpBH3K7",
        "ZOYFDUorctA1",
        "g3yFBqUScrv4",
        "2hxbcHbJ8Rh_",
        "R4WfclFA7wYt",
        "IhBwvatXeDTF"
      ],
      "name": "Image Colorization Vision And Perception.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}