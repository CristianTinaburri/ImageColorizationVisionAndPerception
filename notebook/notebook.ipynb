{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to run\n",
        "Run all cells using the combination CTRL + F9, images will be visualized using the best trained model \"updatedmodel\", the images will be outputed in the \"Visualizing images\" section. You can change the hyperparameters for the training in the hyperparameters section, just set the variable TRAIN to True, same for testing ( TEST variable ).\n",
        "\n",
        "Note: train, test and visualization functions code is taken in part from https://colab.research.google.com/github/smartgeometry-ucl/dl4g/blob/master/colorization.ipynb#scrollTo=YDbY6pf2VU5T and adjusted to fit the use cases of this project."
      ],
      "metadata": {
        "id": "eQoZ5wmzRYeK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjmmMP5cc24I"
      },
      "source": [
        "### ENVIROMENT SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8yCOvNht2f8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb82864-25f7-4024-d039-5ee27022283d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ImageColorizationVisionAndPerception'...\n",
            "remote: Enumerating objects: 60125, done.\u001b[K\n",
            "remote: Counting objects: 100% (60125/60125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 60125 (delta 60045), reused 60087 (delta 60021), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (60125/60125), 20.64 MiB | 13.64 MiB/s, done.\n",
            "Resolving deltas: 100% (60045/60045), done.\n",
            "Checking out files: 100% (60015/60015), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/CristianTinaburri/ImageColorizationVisionAndPerception\n",
        "\n",
        "# Copy contents\n",
        "!cp -a /content/ImageColorizationVisionAndPerception/. /content/\n",
        "\n",
        "# Clean Enviroment\n",
        "!rm -rf /content/ImageColorizationVisionAndPerception\n",
        "!rm -rf /content/sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BqYIpAFG9rsn"
      },
      "outputs": [],
      "source": [
        "# Clean folders\n",
        "!rm -rf /content/training_images\n",
        "!rm -rf /content/testing_images\n",
        "\n",
        "!mkdir /content/training_images\n",
        "!mkdir /content/testing_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsMF5AqXc0cH"
      },
      "source": [
        "### LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z6edOx2GZ_rl"
      },
      "outputs": [],
      "source": [
        "# LIBRARIES\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from skimage import color\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import cv2\n",
        "\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCzW5cpBH3K7"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RksKtBuzH5K7"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 32       # image size\n",
        "num_epochs = 250       # number of epochs to be performed during training\n",
        "batch_size = 50        # batch size\n",
        "learning_rate = 2e-4      # learning rate\n",
        "betas = (0.9, 0.999)      # betas\n",
        "eps = 1e-08       # eps\n",
        "weight_decay = 2.5e-5     # weight_decay\n",
        "\n",
        "use_gpu = True          # use_gpu\n",
        "num_workers = 2          # num_workers\n",
        "TRANSFORM_DATA = False      # apply transform to data\n",
        "DATASET = \"CIFAR\"          # the dataset you want to use for training\n",
        "MODEL_NAME = \"updated_model\" + \".pth\"    # name of model to save\n",
        "PATH_TO_SAVE_MODEL = \"/content/gdrive/MyDrive/Computer Vision Project/Models/End Models/\"   # where to save the model\n",
        "MODEL_PATH_SAVE =  PATH_TO_SAVE_MODEL + MODEL_NAME + \"/\"    # complete variable to where to save the model\n",
        "NUM_EPOCH_MODEL_SAVE = 5       # how many epoch to save model\n",
        "TEST_EPOCHS = 10        # the number of epoch after it need to be tested\n",
        "LOAD_MODEL = True   # if the model needs to be loaded\n",
        "TRAIN = False   # if the model needs to be trained\n",
        "TEST = False    # if the model needs to be tested"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOYFDUorctA1"
      },
      "source": [
        "### Dataset Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m3H_6py1HYTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad906df-ddb8-4067-e753-3fd3973d38d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/modules\n",
            "/content\n",
            "5000\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "%cd modules\n",
        "\n",
        "import utils\n",
        "\n",
        "%cd ..\n",
        "\n",
        "utils.copy_dataset_for_training('datasets/' + DATASET)\n",
        "number_files_training, number_files_testing = utils.get_number_of_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XqVf8TWr3Lv5"
      },
      "outputs": [],
      "source": [
        "if TRANSFORM_DATA == True:\n",
        "  transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Resize((IMG_SIZE,IMG_SIZE)), transforms.ToPILImage()])\n",
        "else:\n",
        "  transform = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JG0rs9xbMxo",
        "outputId": "87bd6b07-3913-4088-d68c-9424f414b0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/modules\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd modules\n",
        "\n",
        "import dataset\n",
        "\n",
        "# Setup data\n",
        "train_set = dataset.ImageDataset(\"/content/training_images\", transform = transform)\n",
        "test_set = dataset.ImageDataset(\"/content/testing_images\", transform = transform)\n",
        "\n",
        "# Setup dataloaders\n",
        "train_dataloader = DataLoader(train_set, num_workers=num_workers, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_set, num_workers=num_workers, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yFBqUScrv4"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "eb95c3e7cff343b1a29d6bfb7f1abf77",
            "0f9ff803773444bfb1a21c4bd2e8f79d",
            "c6b4330c8d6e42c09005dc76502f40ac",
            "a531d61b60d5499cbaf605e7ed97cc93",
            "732b418c95854e3496ca5a5bb15a6072",
            "e1767a552cab4381bb98d0fc56816087",
            "25dbfc4040034455bdf5dcf5275f845f",
            "efd3a3e46fa14468aa7cd48586487f9c",
            "6f0951720e8d4ecdbdce84f82b1b215d",
            "200a4f2a07774b12b2fbc900328602fd",
            "d6789f9ba41f4744a0cf03272a703c3e"
          ]
        },
        "id": "TAI3WRKpxxt6",
        "outputId": "93841456-11aa-4f2b-e1de-f9c0d8ddb9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb95c3e7cff343b1a29d6bfb7f1abf77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# THIS CELL IMPORTS THE MODELS\n",
        "\n",
        "%cd models\n",
        "\n",
        "import colorfulcolorization\n",
        "import basemodel\n",
        "import updatedmodel\n",
        "import colornetblock\n",
        "import ResNetBackbone18\n",
        "\n",
        "colorfulcolorization = colorfulcolorization.colorfulcolorization()\n",
        "basemodel = basemodel.basemodel()\n",
        "updatedmodel = updatedmodel.updatedmodel()\n",
        "ResNetBackbone18 = ResNetBackbone18.ResNetBackbone18(p=0.8)\n",
        "colornetblock = colornetblock.colornetblock(dim=128, percentage_dropout=0.5)\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEC-KC5__gJv",
        "outputId": "a4d64bd2-4150-4311-bc4d-7253b864c01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 128104\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]             640\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "   ConvTranspose2d-4           [-1, 64, 32, 32]           4,160\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
            "              ReLU-7           [-1, 64, 32, 32]               0\n",
            "           Dropout-8           [-1, 64, 32, 32]               0\n",
            "            Conv2d-9           [-1, 64, 32, 32]          36,928\n",
            "      BatchNorm2d-10           [-1, 64, 32, 32]             128\n",
            "             ReLU-11           [-1, 64, 32, 32]               0\n",
            "          Dropout-12           [-1, 64, 32, 32]               0\n",
            "           Conv2d-13           [-1, 32, 32, 32]          18,464\n",
            "      BatchNorm2d-14           [-1, 32, 32, 32]              64\n",
            "             ReLU-15           [-1, 32, 32, 32]               0\n",
            "  ConvTranspose2d-16           [-1, 64, 32, 32]           2,112\n",
            "           Conv2d-17           [-1, 32, 32, 32]          18,464\n",
            "      BatchNorm2d-18           [-1, 32, 32, 32]              64\n",
            "             ReLU-19           [-1, 32, 32, 32]               0\n",
            "          Dropout-20           [-1, 32, 32, 32]               0\n",
            "           Conv2d-21           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-22           [-1, 32, 32, 32]              64\n",
            "             ReLU-23           [-1, 32, 32, 32]               0\n",
            "          Dropout-24           [-1, 32, 32, 32]               0\n",
            "           Conv2d-25            [-1, 2, 32, 32]             578\n",
            "  ConvTranspose2d-26            [-1, 2, 32, 32]               6\n",
            "================================================================\n",
            "Total params: 128,104\n",
            "Trainable params: 128,104\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 9.28\n",
            "Params size (MB): 0.49\n",
            "Estimated Total Size (MB): 9.77\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# THIS CELL INITIALIZE THE CHOOSEN MODEL\n",
        "\n",
        "# Initialize model\n",
        "# CHANGE THIS LINE TO THE MODEL ARCHITECTURE YOU WANT\n",
        "model = updatedmodel\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Port model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Print number of parameters\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % (num_params))\n",
        "\n",
        "# Import libraries to get summary of model architecture\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "\n",
        "# Get summary of model architecture\n",
        "summary(model, (1,32,32))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL == True:\n",
        "  MODEL_PATH_TO_LOAD = \"/content/models/pretrained/\" + MODEL_NAME\n",
        "  model.load_state_dict(copy.deepcopy(torch.load(MODEL_PATH_TO_LOAD, device)))\n",
        "  model.eval()"
      ],
      "metadata": {
        "id": "TJ7su07gKrNS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hxbcHbJ8Rh_"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J0h1BpWI8khY"
      },
      "outputs": [],
      "source": [
        "# Loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, betas=betas, eps=eps, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Bjul7c-QdrNw"
      },
      "outputs": [],
      "source": [
        "if TRAIN == True:\n",
        "  # set to training mode\n",
        "  model.train()\n",
        "\n",
        "  train_loss_avg = []\n",
        "\n",
        "  print('Training Started')\n",
        "  for epoch in range(num_epochs):\n",
        "      train_loss_avg.append(0)\n",
        "      num_batches = 0\n",
        "\n",
        "      if epoch % NUM_EPOCH_MODEL_SAVE == 0 and epoch != 0:\n",
        "        torch.save(model.state_dict(), MODEL_PATH_SAVE + MODEL_NAME + \"_\" + str(int(epoch)) + \".pth\")\n",
        "        pass\n",
        "\n",
        "      if epoch % 10 == 0 and epoch != 0:\n",
        "        utils.save_predictions()\n",
        "        pass\n",
        "      \n",
        "      for l, ab in train_dataloader:\n",
        "          \n",
        "          l, ab = l.to(device), ab.to(device)\n",
        "\n",
        "          predicted_ab_batch = model(l)\n",
        "          \n",
        "          loss = loss_function(predicted_ab_batch, ab)\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          \n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss_avg[-1] += loss.item()\n",
        "          num_batches += 1\n",
        "\n",
        "          if num_batches*batch_size % int(number_files_training/10) == 0 and num_batches > 0:\n",
        "            print(num_batches*batch_size, \" / \", number_files_training)\n",
        "          \n",
        "      train_loss_avg[-1] /= num_batches\n",
        "      print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))\n",
        "\n",
        "  torch.save(model.state_dict(), MODEL_PATH_SAVE + MODEL_NAME + \"_\" + str(int(epoch)) + \".pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4WfclFA7wYt"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xqDv2hk-8UFX"
      },
      "outputs": [],
      "source": [
        "if TEST == True:\n",
        "  model.eval()\n",
        "\n",
        "  test_loss_avg, num_batches = 0, 0\n",
        "\n",
        "  for l, ab in test_dataloader:\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "          l, ab = l.to(device), ab.to(device)\n",
        "\n",
        "          predicted_ab_batch = model(l)\n",
        "          \n",
        "          loss = loss_function(predicted_ab_batch, ab)\n",
        "\n",
        "          test_loss_avg += loss.item()\n",
        "          num_batches += 1\n",
        "      \n",
        "  test_loss_avg /= num_batches\n",
        "  print('average loss: %f' % (test_loss_avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhBwvatXeDTF"
      },
      "source": [
        "### Visualizing images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0XnjNP0eB8O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from skimage import color, io\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "import torchvision.utils\n",
        "\n",
        "number_of_images_grid = 5\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # returns an array of random numbers of size of the variable number_of_images_grid\n",
        "    image_inds = np.random.choice(len(test_set), number_of_images_grid, replace=False)\n",
        "\n",
        "    # return a tensor with random elements of the size of the variable number_of_images_grid\n",
        "    lab_batch = torch.stack([torch.cat([test_set[i][0], test_set[i][1]], dim=0) for i in image_inds])\n",
        "\n",
        "    # ports lab_batch on the gpu\n",
        "    lab_batch = lab_batch.to(device)\n",
        "\n",
        "    # declaring the list that will contain the images to be predicted\n",
        "    predicted_lab_batch = []\n",
        "\n",
        "    # predicts the AB channels of the images using the trained model\n",
        "    predicted_lab_batch = torch.cat([lab_batch[:, 0:1, :, :], model(lab_batch[:, 0:1, :, :])], dim=1)\n",
        "\n",
        "    # ports lab_batch on the cpu\n",
        "    lab_batch = lab_batch.cpu()\n",
        "\n",
        "    # ports the predicted_lab_batch on the cpu\n",
        "    predicted_lab_batch = predicted_lab_batch.cpu()\n",
        "\n",
        "    # visualizing the images\n",
        "    rgb_batch = []\n",
        "    predicted_rgb_batch = []\n",
        "    for i in range(lab_batch.size(0)):\n",
        "        predicted_rgb_img = color.lab2rgb(np.transpose(predicted_lab_batch[i, :, :, :].numpy().astype('float64'), (1, 2, 0)))\n",
        "        predicted_rgb_batch.append(torch.FloatTensor(np.transpose(predicted_rgb_img, (2, 0, 1))))\n",
        "\n",
        "        rgb_img = color.lab2rgb(np.transpose(lab_batch[i, :, :, :].numpy().astype('float64'), (1, 2, 0)))\n",
        "        rgb_batch.append(torch.FloatTensor(np.transpose(rgb_img, (2, 0, 1))))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(30, 30), nrows=1, ncols=2)\n",
        "    ax[0].imshow(np.transpose(torchvision.utils.make_grid(torch.stack(predicted_rgb_batch), nrow=5).numpy(), (1, 2, 0)))\n",
        "    ax[0].title.set_text('Predicted')\n",
        "    ax[1].imshow(np.transpose(torchvision.utils.make_grid(torch.stack(rgb_batch), nrow=5).numpy(), (1, 2, 0)))\n",
        "    ax[1].title.set_text('Reference')\n",
        "\n",
        "    # if you want to save the predicted images\n",
        "    # plt.savefig(MODEL_PATH_SAVE + MODEL_NAME + '_' + str(250) + '.png', bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LjmmMP5cc24I",
        "WsMF5AqXc0cH",
        "XCzW5cpBH3K7",
        "ZOYFDUorctA1",
        "g3yFBqUScrv4",
        "2hxbcHbJ8Rh_",
        "R4WfclFA7wYt"
      ],
      "name": "Image Colorization Vision And Perception.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb95c3e7cff343b1a29d6bfb7f1abf77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f9ff803773444bfb1a21c4bd2e8f79d",
              "IPY_MODEL_c6b4330c8d6e42c09005dc76502f40ac",
              "IPY_MODEL_a531d61b60d5499cbaf605e7ed97cc93"
            ],
            "layout": "IPY_MODEL_732b418c95854e3496ca5a5bb15a6072"
          }
        },
        "0f9ff803773444bfb1a21c4bd2e8f79d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1767a552cab4381bb98d0fc56816087",
            "placeholder": "​",
            "style": "IPY_MODEL_25dbfc4040034455bdf5dcf5275f845f",
            "value": "100%"
          }
        },
        "c6b4330c8d6e42c09005dc76502f40ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efd3a3e46fa14468aa7cd48586487f9c",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f0951720e8d4ecdbdce84f82b1b215d",
            "value": 46830571
          }
        },
        "a531d61b60d5499cbaf605e7ed97cc93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_200a4f2a07774b12b2fbc900328602fd",
            "placeholder": "​",
            "style": "IPY_MODEL_d6789f9ba41f4744a0cf03272a703c3e",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 180MB/s]"
          }
        },
        "732b418c95854e3496ca5a5bb15a6072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1767a552cab4381bb98d0fc56816087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25dbfc4040034455bdf5dcf5275f845f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efd3a3e46fa14468aa7cd48586487f9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f0951720e8d4ecdbdce84f82b1b215d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "200a4f2a07774b12b2fbc900328602fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6789f9ba41f4744a0cf03272a703c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}